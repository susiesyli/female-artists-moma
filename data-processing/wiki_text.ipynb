{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "def search_wikipedia_api(display_name):\n",
    "    \"\"\"\n",
    "    Search Wikipedia API for an artist using their display name\n",
    "    \"\"\"\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    # Construct search query - try with and without \"artist\" qualifier\n",
    "    search_terms = [\n",
    "        f\"{display_name} artist\",\n",
    "        f\"{display_name} painter\",\n",
    "        f\"{display_name}\"\n",
    "    ]\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'ArtistResearchBot/1.0 (your@email.com)'\n",
    "    }\n",
    "    \n",
    "    for search_term in search_terms:\n",
    "        try:\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"format\": \"json\",\n",
    "                \"list\": \"search\",\n",
    "                \"srsearch\": search_term,\n",
    "                \"srlimit\": 1\n",
    "            }\n",
    "            \n",
    "            response = requests.get(search_url, params=params, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if data[\"query\"][\"search\"]:\n",
    "                page_id = data[\"query\"][\"search\"][0][\"pageid\"]\n",
    "                \n",
    "                # Get page content\n",
    "                params = {\n",
    "                    \"action\": \"query\",\n",
    "                    \"format\": \"json\",\n",
    "                    \"prop\": \"extracts|categories\",\n",
    "                    \"exintro\": True,\n",
    "                    \"explaintext\": True,\n",
    "                    \"pageids\": page_id\n",
    "                }\n",
    "                \n",
    "                content_response = requests.get(search_url, params=params, headers=headers)\n",
    "                content_response.raise_for_status()\n",
    "                content_data = content_response.json()\n",
    "                \n",
    "                page_content = content_data[\"query\"][\"pages\"][str(page_id)]\n",
    "                \n",
    "                # Check if the page is about an artist by looking at categories\n",
    "                categories = page_content.get(\"categories\", [])\n",
    "                is_artist = any(\"artist\" in str(cat).lower() or \n",
    "                              \"painter\" in str(cat).lower() or \n",
    "                              \"sculptor\" in str(cat).lower() \n",
    "                              for cat in categories)\n",
    "                \n",
    "                if is_artist:\n",
    "                    return {\n",
    "                        \"extract\": page_content[\"extract\"],\n",
    "                        \"page_id\": page_id,\n",
    "                        \"confidence\": \"high\" if \"artist\" in search_term else \"medium\"\n",
    "                    }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for {display_name}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return {\"extract\": \"\", \"page_id\": None, \"confidence\": \"none\"}\n",
    "\n",
    "def process_artists_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process CSV file containing artist information\n",
    "    Expected column: display_name (in format \"Firstname Lastname\")\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Create empty lists to store results\n",
    "    extracts = []\n",
    "    page_ids = []\n",
    "    confidence_levels = []\n",
    "    \n",
    "    # Process each artist with a progress bar\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing artists\"):\n",
    "        result = search_wikipedia_api(row['DisplayName'])\n",
    "        extracts.append(result['extract'][:500] if result['extract'] else '')  # Limit to first 500 chars\n",
    "        page_ids.append(result['page_id'])\n",
    "        confidence_levels.append(result['confidence'])\n",
    "        \n",
    "        # Be nice to Wikipedia's servers\n",
    "        # sleep(1)\n",
    "    \n",
    "    # Add new columns to the dataframe\n",
    "    df['wikipedia_extract'] = extracts\n",
    "    df['wikipedia_page_id'] = page_ids\n",
    "    df['match_confidence'] = confidence_levels\n",
    "    \n",
    "    # Save the results\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    total = len(df)\n",
    "    found = sum(1 for x in page_ids if x is not None)\n",
    "    print(f\"\\nStats:\")\n",
    "    print(f\"Total artists processed: {total}\")\n",
    "    print(f\"Artists found: {found} ({(found/total)*100:.1f}%)\")\n",
    "    print(f\"Artists not found: {total - found} ({((total-found)/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing artists: 100%|██████████| 2435/2435 [33:23<00:00,  1.22it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to female_artists_with_work_counts_wiki.csv\n",
      "\n",
      "Stats:\n",
      "Total artists processed: 2435\n",
      "Artists found: 1617 (66.4%)\n",
      "Artists not found: 818 (33.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_file = \"female_artists_with_work_counts.csv\"\n",
    "    output_file = \"female_artists_with_work_counts_wiki.csv\"\n",
    "    process_artists_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/susiesyli/Desktop/poetic-data-final/data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cc7239228f96f60021c839d45e73f0469089d31d4a341e8e753c0ae0a151559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
